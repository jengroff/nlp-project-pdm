{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python391jvsc74a57bd0aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49",
      "display_name": "Python 3.9.1 64-bit"
    },
    "colab": {
      "name": "Get-top-brands-and-sentiment-in-study.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "70JFG7cXnc3n"
      ]
    },
    "metadata": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRcieG09k7Ru"
      },
      "source": [
        "# CODE - Click the PLAY button"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gasvzK-XBTL-"
      },
      "source": [
        "import requests \n",
        "import string\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "import json\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import openpyxl\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "\n",
        "def get_bearer_token():\n",
        "    url = \"https://api.looklook.app/api/auth/login\"\n",
        "    payload = json.dumps({\n",
        "          \"email\": \"josh+admin@spark-nyc.com\",\n",
        "          \"password\": \"Josh740808!\"\n",
        "          })\n",
        "    headers = {\n",
        "          'Content-Type': 'application/json', \n",
        "          'Cookie': 'adminBackdoor=Bearer%20eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VySWQiOiIxYzUwMWZmYS0wMDhkLTQyMjItYWUxZC1kOGE2Y2E5NWJhYTYiLCJpYXQiOjE2MTc3MjQ2NDEsImV4cCI6MTYzMzI3NjY0MSwiYXVkIjoiYWRtaW4ubG9va2xvb2suYXBwIiwiaXNzIjoiTG9va0xvb2tBcHAiLCJzdWIiOiJsb29rbG9va0BzcGFyay1ueWMuY29tIn0.OIld-4ZEyWDfHdiK9oM0EH7qfu095hKn9uVzh5Ppf-xphn5pcdVjNWQZuNeHZQ6LBXzVVAX-CShoRb7qA8PXAWKPE7okv4Lo_SPmjkl5p7Xv7QJqUnuUNAjjB0HEPWo6Hn3ApAgyzdtNpKpoWzE6er6hK_AtSg9fvQ6YdjdboXAWGa6m5BVJVDzSc4x19K7K4nWUl5SfDnkvomkHDvqwzewJbig-Zjbwn_VjHtFfn_v-E2UAiCEFhGgtOS-ImEFZ_E3wHeCuhkC6T9AZOGdasaGAGU9XInZs4tNmeX5yg9_AUj2VgQ6xCkB_B8mI3dcimDG90Yd5P6v5JtGpR0_Q8g; AWSALB=CbiwyfR8G0LGI8kwSXMmERpnu8AfbYK929rTC5/r8iAs0mCgLWVMW619FAeuF6ObpIEpPcdmorCK72mgCw3q6KZs4JsI8yaJ5QC6gNLYnf75hziVY3s1ilsPWbix; AWSALBCORS=CbiwyfR8G0LGI8kwSXMmERpnu8AfbYK929rTC5/r8iAs0mCgLWVMW619FAeuF6ObpIEpPcdmorCK72mgCw3q6KZs4JsI8yaJ5QC6gNLYnf75hziVY3s1ilsPWbix'\n",
        "          }\n",
        "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
        "    response = response.json() \n",
        "    token = response['token']\n",
        "    return token \n",
        "\n",
        "bearer = get_bearer_token()\n",
        "payload={}\n",
        "headers = {\n",
        "  'Authorization': bearer,\n",
        "  'Cookie': 'adminBackdoor=Bearer%20eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VySWQiOiIxYzUwMWZmYS0wMDhkLTQyMjItYWUxZC1kOGE2Y2E5NWJhYTYiLCJpYXQiOjE2MTc2MDc5NjQsImV4cCI6MTYzMzE1OTk2NCwiYXVkIjoiYWRtaW4ubG9va2xvb2suYXBwIiwiaXNzIjoiTG9va0xvb2tBcHAiLCJzdWIiOiJsb29rbG9va0BzcGFyay1ueWMuY29tIn0.MOFmAspJ6KCxgNHmm-RYmMldLYoLnzVL3PDyZlLSD69sSwGfHIJC4w8Jpz90sNi7LFbSx-OgHFaV7ogTh8fKLKoCkAsSrw3ixeAJOq6y0pvQdnyBOCf_QfSYZXfiH4HHciNwGzp7WmIgK1z6AVo3mOJL4ThKvhy2pH4mnYC8MPhjVw38EWJ37fZhF0uBu45xQxPKA9oWcxMXZpzGMZhr0eeY3N0CMJzGWJRlg3WqK6Z-7xGAj7us7cB5HvTOdnx-PR8o1BErBY2wtoi-YKqyhP5s3tzUjmc-cm1fxgn1Wd1sIZXi-Dpc-jb1st442W8-DHcFPTqZ13D6Fa6Z9Y4SEw; AWSALB=bP6czL5Ru/rQmphYq36DMvT6DxykC6HTtYkDLG2UV1Pb4VbKmandS5Z+O0mluiBbbavQJH5hdCBZyrGLQKycsgILiXfBpZ4eD/Ev/FP902YxYavMELYU4kJUS6jT; AWSALBCORS=bP6czL5Ru/rQmphYq36DMvT6DxykC6HTtYkDLG2UV1Pb4VbKmandS5Z+O0mluiBbbavQJH5hdCBZyrGLQKycsgILiXfBpZ4eD/Ev/FP902YxYavMELYU4kJUS6jT'\n",
        "}\n",
        "\n",
        "\n",
        "def get_study_name(studyId: int):\n",
        "    url = f\"https://api.looklook.app/api/studies/{studyId}\"\n",
        "    response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
        "    json_response = response.json()\n",
        "    study_name = json_response['study']['name']\n",
        "    return study_name\n",
        "\n",
        "\n",
        "def get_list_of_teams():\n",
        "    url = \"https://api.looklook.app/api/teams\"\n",
        "    response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
        "    response = response.json()\n",
        "    for i in response['teams']:\n",
        "        print(f\"Name: {i['name']}\")\n",
        "        print(f\"ID: {i['id']}\")\n",
        "        print(f\"Parent ID: {i['parentId']}\\n\")\n",
        "\n",
        "\n",
        "def get_list_of_studies(teamId: int):\n",
        "    url = f\"https://api.looklook.app/api/studies?perPage=100&page=1&teamId={teamId}\"\n",
        "    response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
        "    json_response = response.json()\n",
        "    for i in json_response['studies']:\n",
        "        print(f\"Name:\\t\\t{i['name']}\")\n",
        "        print(f\"id:\\t\\t{i['id']}\")\n",
        "        print(f\"Participants:\\t{i['meta']['participantCount']}\")\n",
        "        print(f\"Topics:\\t\\t{i['meta']['topicCount']}\")\n",
        "        print(f\"Questions:\\t{i['meta']['questionCount']}\")\n",
        "        print(f\"Groups:\\t\\t{i['groups']}\\n\")\n",
        "\n",
        "\n",
        "def get_brands_in_study(studyId: int):\n",
        "    url = f\"https://api.looklook.app/api/studies/{studyId}/analytics?layout=10&questionTypeFilter[]=10&includeModeratorReplies=false\"\n",
        "    response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
        "    result = response.json()\n",
        "\n",
        "    def json_extract(obj, key):\n",
        "        \"\"\"Recursively fetch values from nested JSON.\"\"\"\n",
        "        arr = []\n",
        "\n",
        "        def extract(obj, arr, key):\n",
        "            \"\"\"Recursively search for values of key in JSON tree.\"\"\"\n",
        "            if isinstance(obj, dict):\n",
        "                for k, v in obj.items():\n",
        "                    if isinstance(v, (dict, list)):\n",
        "                        extract(v, arr, key)\n",
        "                    elif k == key:\n",
        "                        arr.append(v)\n",
        "            elif isinstance(obj, list):\n",
        "                for item in obj:\n",
        "                    extract(item, arr, key)\n",
        "            return arr\n",
        "\n",
        "        values = extract(obj, arr, key)\n",
        "        return values\n",
        "\n",
        "    study_text = json_extract(result, \"text\")\n",
        "\n",
        "    words = ''.join([word for word in study_text if not word.isdigit()])\n",
        "\n",
        "    words_raw = len(words)\n",
        "    print(f\"Number of words in study before cleaning: {words_raw}\")\n",
        "\n",
        "    words = ' '.join(word.strip(punctuation) for word in words.split() if word.strip(punctuation))\n",
        "    words = words.replace('_', '')\n",
        "    words = words.replace('?', '')\n",
        "    words = words.replace('•', '')\n",
        "    words = words.replace(\"@\", '')\n",
        "    words = words.replace('▯', '')\n",
        "    words = words.replace(\"'\", '')\n",
        "    words = words.replace(\",\", \"\")\n",
        "    words = words.replace(\"ó\", '')\n",
        "    words = words.replace(\"Ä\", '')\n",
        "    words = words.replace(\"ô\", '')\n",
        "    words = words.replace(\"ò\", '')\n",
        "    words = words.replace(\"ô\", '')\n",
        "    words = words.replace(\"õ\", '')\n",
        "    words = words.replace(\"id\", '')\n",
        "    words = words.replace(\"im\", '')\n",
        "    words = words.replace(\"get\", '')\n",
        "    words = words.replace(\"cla\", '')\n",
        "    words = words.replace(\"IÕve\", '')\n",
        "    words = words.replace(\"Cov\", '')\n",
        "    words = words.replace(\"cov\", '')\n",
        "    words = words.replace(\"COVID\", '')\n",
        "    words = words.replace(\"dn't\", '')\n",
        "    words = words.replace(\"Ill\", '')\n",
        "    words = words.replace(\"CBD\", '')\n",
        "    words = words.replace(\"app\", '')\n",
        "    words = words.replace(\"<p>\", '')\n",
        "    words = words.replace(\"</p>\", '')\n",
        "    words = words.replace(\"p><p\", '')\n",
        "    words = words.replace(\"p>\", '')\n",
        "    words = words.replace(\"e.g\", '')\n",
        "    words = words.replace(\"Malinda\", '')\n",
        "    words = words.replace(\"Unless I'm\", '')\n",
        "    words = words.replace('rel=\"noopener\"', '')\n",
        "\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    doc = nlp(words)\n",
        "    words_cleaned = len(doc)\n",
        "    print(f\"Number of words in study after cleaning: {words_cleaned}\")\n",
        "\n",
        "    reduction = ((1 - (words_cleaned / words_raw)) * 100)\n",
        "    reduction = int(reduction)\n",
        "    print(f\"Cleaning reduction: {reduction}%\\n\")\n",
        "    \n",
        "    lst = []\n",
        "    for ent in doc.ents:\n",
        "            if ent.label_ == 'PRODUCT' or ent.label_ == 'ORG':\n",
        "                lst.append(ent.text)\n",
        "\n",
        "    counts = Counter(lst).most_common(50)\n",
        "    df = pd.DataFrame(counts, columns =['text', 'count'])\n",
        "    \n",
        "    study_name = get_study_name(studyId)\n",
        "\n",
        "    print(f\"Top 25 brands (and related words) mentioned in '{study_name}':\\n\")\n",
        "    print(df.head(50))\n",
        "\n",
        "\n",
        "def get_study_sentiment(studyId: int, keyword: str = None, score: int = -10.00, export: bool = False):\n",
        "    url = f\"https://api.looklook.app/api/studies/{studyId}/analytics?layout=10&questionTypeFilter[]=10&includeModeratorReplies=false\"\n",
        "    response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
        "    result = response.json()\n",
        "\n",
        "    def json_extract(obj, key):\n",
        "        arr = []\n",
        "\n",
        "        def extract(obj, arr, key):\n",
        "            if isinstance(obj, dict):\n",
        "                for k, v in obj.items():\n",
        "                    if isinstance(v, (dict, list)):\n",
        "                        extract(v, arr, key)\n",
        "                    elif k == key:\n",
        "                        arr.append(v)\n",
        "            elif isinstance(obj, list):\n",
        "                for item in obj:\n",
        "                    extract(item, arr, key)\n",
        "            return arr\n",
        "\n",
        "        values = extract(obj, arr, key)\n",
        "        return values\n",
        "\n",
        "    study_text = json_extract(result, \"text\")\n",
        "    ids = json_extract(result, \"id\")\n",
        "    text_with_ids_tuples = list(zip(ids, study_text))\n",
        "    df = pd.DataFrame(text_with_ids_tuples, columns=['ID', \"Response\"])\n",
        "\n",
        "    pd.set_option('display.max_colwidth', 200)\n",
        "    pd.set_option('display.colheader_justify', 'center')\n",
        "\n",
        "    df['Response'] = df['Response'].apply(lambda x: x.replace('<p>', ''))\n",
        "    df['Response'] = df['Response'].apply(lambda x: x.replace('</p>', ''))\n",
        "    df = df.set_index('ID')\n",
        "\n",
        "    analyzer = SentimentIntensityAnalyzer()\n",
        "    df['Score'] = df['Response'].apply(lambda response: analyzer.polarity_scores(response)['compound'])\n",
        "    df.sort_values(by=['Score'], ascending=False, inplace=True)\n",
        "    grouped = df.groupby('ID')\n",
        "\n",
        "    score = score / 10.00\n",
        "    if score > 0.00: df = grouped.filter(lambda x: x['Score'] > score)\n",
        "    elif score == -1.00: df = grouped.filter(lambda x: x['Score'] > score)\n",
        "    else: df = grouped.filter(lambda x: x['Score'] < score)\n",
        "  \n",
        "    grouped = df.groupby('ID')\n",
        "    \n",
        "    if keyword != None: df = grouped.filter(lambda x: x['Response'].str.contains(keyword)); df['keyword'] = keyword \n",
        "    else: pass\n",
        "\n",
        "    df['Score'] = df['Score'].apply(lambda x: x * 10)\n",
        "\n",
        "    def get_study_name(studyId: int):\n",
        "        url = f\"https://api.looklook.app/api/studies/{studyId}\"\n",
        "        response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
        "        json_response = response.json()\n",
        "        study_name = json_response['study']['name']\n",
        "        return study_name\n",
        "\n",
        "    if export == True: \n",
        "        study_name = get_study_name(studyId)\n",
        "        df.to_csv(f'{study_name}_export.csv')\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gh66IgL7mIfD"
      },
      "source": [
        "# TOP 25 BRANDS mentioned in a Study "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYhQ5NIekcmE"
      },
      "source": [
        "get_brands_in_study(39)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70JFG7cXnc3n"
      },
      "source": [
        "# STUDY SENTIMENT\n",
        "\n",
        "## Things you can get:\n",
        "*   All Participant responses, ranked by sentiment (from positive to negative)\n",
        "*   ALL POSITIVE responses above a score you specify\n",
        "*   ALL NEGATIVE responses below a score you specify\n",
        "*   MOST POSITIVE responses associated with a SPECIFIC BRAND\n",
        "*   MOST NEGATIVE responses associated with a SPECIFIC BRAND\n",
        "*   ANY MENTION of a particular BRAND (or any keyword)\n",
        "*   .CSV EXPORT of any of the above\n",
        "\n",
        "## SENTIMENT SCORES\n",
        "*   Range from **-10.00 (most negative)** to **10.00 (most positive)**\n",
        "\n",
        "---\n",
        "\n",
        "## EXAMPLES:\n",
        "**All Participant responses, ranked by sentiment** (from positive to negative)\n",
        "> `get_study_sentiment(studyId=46)`\n",
        "\n",
        "**ALL POSITIVE responses above a certin score**\n",
        "> `get_study_sentiment(studyId=46, score=9.5)`\n",
        "\n",
        "**ALL NEGATIVE responses below a certin score**\n",
        "> `get_study_sentiment(studyId=46, score=-9.5)`\n",
        "\n",
        "**MOST POSITIVE responses associated with a SPECIFIC BRAND**\n",
        "> `get_study_sentiment(studyId=46, score=9.5, keyword=\"Chanel\")`\n",
        "\n",
        "**MOST NEGATIVE responses associated with a SPECIFIC BRAND**\n",
        "> `get_study_sentiment(studyId=46, score=-9.5, keyword=\"Chanel\")`\n",
        "\n",
        "**ANY MENTION of a particular BRAND or KEYWORD**\n",
        "> `get_study_sentiment(studyId=46, keyword=\"Grace Kelly\")`\n",
        "\n",
        "**.CSV EXPORT of any of the above**\n",
        "> `get_study_sentiment(studyId=46, score=9.5, keyword=\"Chanel\", export=True)`\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EW3YbOnkcmF"
      },
      "source": [
        "get_study_sentiment(studyId=39)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ha2UBIMxlV2E"
      },
      "source": [
        "# ALL TEAMS on the Platform "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSN2vtS7laAw"
      },
      "source": [
        "get_list_of_teams()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnXbxUadlfb0"
      },
      "source": [
        "# ALL STUDIES for a specific Team"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaQFuWXxliiR"
      },
      "source": [
        "get_list_of_studies(15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tl9dxDAgp3UP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}